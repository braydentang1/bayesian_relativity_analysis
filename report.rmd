---
title: "A Fully Bayesian Way of Estimating Insurance Relativities"
author: "Brayden Tang"
date: "27/11/2020"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(rstan)
library(tidyverse)
library(recipes)
library(DT)
library(kableExtra)
library(prettydoc)

# CHANGE THIS BEFORE KNITING IF YOU WANT TO RERUN THE MCMC CHAINS.
refit <- FALSE
```

## Some Context
I am not an actuarial student (anymore) but I recently completed a typical
pricing assignment as part of an evaluation for a job application.
I didn't end up applying, however I enjoyed having a familiar (and fake) kind
of dataset that I used to work with all the time in my past life. I wanted to see
if I could get around some of the issues I had with these kinds of datasets
from three years ago.

The objective of actuarial relativity analysis is, at a high level, to most 
accurately predict the pure premium as closely as possible for all combinations of rating variables. The pure premium is the amount of money needed, on average, that is required to cover the cost of
claims only (so no profit allocations or expenses). Rating variables are simply policyholder characteristics (such as the amount of kilometers on a vehicle, age of the policyholder, color of the vehicle, engine size, etc.)

Naturally, pure premium is expressed simply as the rate at which a risk
makes any kind of claim multiplied by the average amount per each claim. Thus, pure premium
is defined as: 

$$ \text{frequency} \times \text{severity} = \frac{\text{total claim counts}}{\text{total risks = exposures}} \times \frac{\text{total claim amounts (losses)}}{\text{total claim counts}} $$ 

$$ = \frac{\text{total claim amounts (losses)}}{\text{total risks}}.$$

We can model either the frequency and severity distributions separately, or we 
can model pure premium directly. The former approach is often preferred as 
it offers more flexibility and is at least logically, more robust. 

## Relativities

Relativities are simply the marginal impact a particular rating variable has on the
response. This has the exact same interpretation as regression coefficients do
in a multivariate linear model - holding all other variables constant, the regression
coefficient represents the change in some quantity (say, the change in the log of severity or
the log of frequency) per some change in said rating variable. 

Relativities are typically calculated by predicting pure premiums across all of the levels of a particular rating variable, while simultaneously holding all of the other rating variables constant at the levels of some chosen base class. The base class represents one specific combination of rating variables that all other classes are compared against. Other combinations of rating variables are more risky (higher pure premium) or less risky (lower pure premium), **relative** to this base class.

Thus, each predicted pure premium is divided by the predicted pure premium of the relevant level from the base class to
obtain these relativities.

```{r relativities, echo = FALSE}

example_city_rels <- tibble(
	City = c("Small City", "Medium City", "Large City"),
	Relativity = c(0.95, 1, 1.50)
	) %>%
	kable() %>%
	kable_styling()

example_km_rels <- tibble(
	Kilometres = c(
		"Less Than 10,000km",
		"10,000-50,000km",
		"50,001-100,000km",
		"More Than 100,000km"
		),
	Relativity = c(0.35, 0.76, 1, 1.378)
) %>%
	kable() %>%
	kable_styling()

knitr::kables(
	list(example_city_rels, example_km_rels),
	caption = "Figure 1: Example relativities with two rating variables") %>% 
	kable_styling() 

```
<br>
Note that the vehicle described by the levels of each rating variable with relativities 1 (medium city & 50,001-100,000km) is the base class (in a linear model, this is the class described by the intercept). Relative to the base class, other vehicles have higher or lower premiums. The base class premium is multiplied by the corresponding relativities
to derive the premiums for the other vehicle classes.

## The Dataset

I import the dataset below:

```{r Import the data, echo = FALSE}

data <- read_csv("data/pricingdat.csv") 

data %>% 
	datatable(
		rownames = FALSE,
		options = list(scrollY = 300, searching = FALSE, scrollX = 300)) %>%
	formatRound(columns = 6:7, digits = 0)

```
<br>
As is typical, one row represents one unique combination of rating variables, 
containing the total number of claims observed, amount of exposures (i.e. number of risks,
which is typically defined as the amount of policy earned over a period of one year) and
the total amount of claim payments. 

While this dataset seems straightforward, it is deceivingly difficult to work with when compared to 
other more standard datasets. This is primarily due to the aggregation which causes rows and
columns to be dependent. If we add more rating variables (which are columns) to the dataset, 
we naturally will get more rows since there will be more unique combinations of rating variables. 
In fact, the number of rows will grow exponentially (see the curse of dimensionality).

The consequences of this are dire. For one, model validation becomes very difficult.
We can not naively split the aggregated dataset above because
each row is unique. Therefore, any possible test set will contain unique combinations of
rating variables that will be simply unobserved in the training set (and vice-versa).

Second, feature selection (which is emphasized a lot in pricing since there is strong
desire to have as simple of a rating algorithm as possible) and therefore, 
model selection, is impossible. We cannot simply drop columns without affecting the number of rows. Thus,
any sort of criteria based on the likelihood function (such as AIC/BIC/WAIC/LOO)
are not viable since the likelihood function naturally scales with the data. In addition,
since we cannot perform external validation we naturally cannot use wrapper based
methods (like forward selection) that rely on held out datasets.

Third, each row will become more sparse as well due to the curse of dimensionality,
with some combinations of rating variables simply having zero claims or exposures. Naturally, this will
lead to ill-defined models - the traditional gamma won't work, for example, since
we will have exact zero losses. In the case of frequency distributions, there
may also be particular levels of a rating variable that simply have zero exposures.
This typically arises when a particular level that originally had some exposures many years ago decreases
over time to the point where it no longer has any exposures (suppose a vehicle color simply ceases to exist within a book of business). Normally, this level can simply be dropped, but
what happens when all of a sudden the exposure reappears in the future? Our data 
is a single realization of some reality, but there is of course **uncertainty**.
It could be by pure chance that we observed zero exposures.

Another solution to the above problem is to group the sparse level of the rating
variable with another to remove this sparsity. This seems harmless, but in reality
this corresponds to a deceivingly strong prior, namely that the
effects of each level on the response are the same. In math, this corresponds to setting $\beta_{1} - \beta_{2} \sim N(0, 2\epsilon)$ where $\epsilon$ is some number very close to 0, and $\beta_{1}, \beta_{2}$ are the effects of the two separate levels being grouped. See [this video](https://www.youtube.com/watch?v=BKumW2RfSoQ&ab_channel=Stan) for more details. Regardless, grouping levels of rating variables together is a very strong prior that
is not at all transparent. Rather than doing this, we can use mixed effects/hierarchical models to achieve partial pooling, which is far more flexible and robust.

### Aside: The Ideal Dataset

The ideal dataset, at least hypothetically, would be a dataset where one row is 
equal to one policyholder. The total number of earned exposures for the policyholder
would be one column, and the total claim counts and total claim costs (for that
policyholder) would be additional columns (from which we could model). The rating
variables relevant for that policyholder would then be represented as additional
columns.

This would secure independence between the number of rating variables used
and the number of rows. To obtain relativities, the aggregated dataset could be recreated
(that is, one row is equal to one unique combination of exposures) from which we could then make our predictions of frequency and severity using our trained model (that was trained on the policyholder data). The model
predicts it's best guess of $E[Y | X = \text{rating variables}]$ learned over all
observed policyholders.

## Post-Adjustments

The model is not perfect and there is an acceptance that the models may be overfit
to the data, leading to relativites that are not reflective of the actual relativites.
This often leads to actuaries manually adjusting relativites to be more in line
with what their apriori beliefs are. For example, a vehicle with a larger engine would
be expected to be of higher risk than a vehicle with a smaller engine since presumably
the vehicles with larger engines are more expensive on average. Sometimes, the 
relativities produced do not reflect this and so an actuary may decide to "manually"
adjust the values to smooth said estimates. This is often done out of respect for the
customer who may not appreciate being charged more in premium when they perceive
themselves as lower risk. Depending on the actuary, they may just adjust relativities that are not monotonic. 

While this approach may sound sketchy, I believe there may actually be merit to it
in the sense that the relationship that is expected, or, a reasonable compromise to the prior belief
**is** actually in the data once we consider the amount of uncertainty that exists within the data. This is 
essentially what this manual adjustment is trying to account for, accepting that
the data has some degree of noise and may not be fully reliable. 

However, the adjustment is arbitrary if we do not actually know how much uncertainty we are 
dealing with. In the case of traditional pricing modeling, it is difficult to actually
quantify how much uncertainty we are dealing with because the frequency and severity
models are estimated completely independently of each other. The expectations produced
by each individual model are used to produce the pure premiums, which are then used to 
calculate relativities. 

However, it is clear that if we view frequency and severity as random variables
that they are not independent. If we have a zero frequency, we must have a zero severity (or equivalently,
if we have observed claims we must have non-zero severity, assuming that claims 
below a deductible are ignored). Estimating both models separately ignores this.

Viewing frequency and severity as random variables, and as a consequence, viewing pure premiums
and relativities as random variables, is where Bayesian modeling will allow us
to quantify the full amount of uncertainty in our data. This will allow actuaries to actually
choose reasonable values that are in line with the amount of uncertainty that they
have apriori and within the data. In addition, it allows actuaries to quantify 
their confidence in allowing for a non-monotonic relativity, giving them the ability
to defend any non-monotonic relativities.

## The Bayesian/Generative Model

I describe the fully generative model below:

Let $X$ be the severity random variable, $R_{i}$ be the rating variables,
and $N$ be the claim counts. Assume that exposures are a known constant. 

Then, for the ith unique combination of rating variables, $\forall i \ X_{i} > 0$ let:

$$X_{i} \big|N_{i}, \phi, \eta_{i} \sim Gamma\left(\frac{N_{i}}{\phi}, \frac{N_{i}}{\eta_{i} \times \phi}\right),$$

$$\eta_{i} \big| \beta, R_{i} = exp(R_{i} \times \beta),$$
$$N_{i} \big|\lambda_{i} \sim Poisson(\lambda_{i}),$$
$$\lambda_{i} \big|\psi, R_{i} = exp(R_{i} \times \psi + log(\text{exposure}_{i})),$$
$$\beta \sim Normal(0, \tau),$$
$$\psi \sim Normal(0, \omega),$$
$$\phi \sim \text{Half-Cauchy}(0, \alpha).$$
Note that $\alpha, \tau, \omega$ are constants that must be specified by the user.
In addition, note that $$E[X_{i} \big |N_{i}, \phi, \eta_{i}] = \frac{N_{i}}{\phi} \times \frac{\eta_{i} \times \phi}{N_{i}} = \eta_{i}$$ but $$Var[X_{i} \big |N_{i}, \phi, \eta_{i}] = \frac{N_{i}}{\phi} \times \frac{\eta_{i}^2 \times \phi^2}{N_{i}^2} = \frac{\eta_{i}^2 \times \phi}{N_{i}}.$$

That is, as the number of claims that makes up a severity calculation increases,
the lower the variance of the resulting severity distribution for the particular
combination of rating variables. Equivalently, the more claims for a particular 
combination of rating variables, the more influence that particular combination
of rating variables has on the overall model fit. This is pretty much the same
thing as using weights in a gamma generalized linear model, however, in this case
the weights are also random variables and everything is modeled simultaneously.

It follows that $$X_{i} \times \frac{N_{i}}{\text{number of risks for ith combination}} = P_{i},$$ where 
$P_{i}$ is the pure premium for the ith combination of rating variables, is also a random variable. Since
we will have joint posterior draws of the two random variables that the pure premium is a function of,
the posterior distribution of the pure premium (and relativities) is also known.

In addition, this Bayesian approach helps solve problems related to the aggregation 
of data. 

### Feature Selection

It was mentioned previously that feature selection is very difficult to carry out
on an aggregated dataset where rows and columns are not independent. This is true
if we are using explicit feature selection where rating variables are literally removed
from the design matrix/training dataset.

However, we can achieve feature selection through regularization as well. It turns
out that L1 (LASSO), and L2 (ridge) are equivalent to Laplace and Normal zero mean priors on regression
coefficients, for example. L1 regularization actually has the potential to explicitly 
feature select as well, allowing for exactly zero coefficients (albeit, we are not maximizing but averaging in MCMC so we won't ever get this exactly in the Bayesian approach). The cost parameter
typically associated with these regularization techniques is exactly equal to the 
variance of the Laplace/Normal distributions - the larger the variance, the lower the cost/regularization.
Bayesian approaches also allow for even more specific priors depending on the context, such as the
horseshoe/spike and slab priors.

In the Bayesian context, the variance is chosen based on our confidence that the (predictive)
effects of each level are non-zero, apriori. Alternatively, one can choose to use an
empirical Bayes approach and tune this variance parameter using something like cross validation.
This is equivalent to the typical machine learning approach - but in the Bayes context this isn't
as feasible since Bayesian models using MCMC are slow. An alternative is to use a Bayesian
estimate of out of sample log pointwise predictive density (ELPD) using importance sampling (counterfactuals of the loglikelihood), but this as mentioned earlier is a problem with our aggregated dataset.

Regardless, the Bayesian model allows us to use regularization while still remaining
completely probabilistic. This will pay off when we look at relativities.

### Predictive Performance

The other main issue involved the issue of model validation in terms of predictive 
performance. We cannot just naively split the dataset nor can we use likelihood based 
evaluation metrics, like ELPD (since the dataset has rows as a function of the number of columns).

Unfortunately, I still haven't figured out a great way to solve this issue. The
only alternative may be to perform posterior predictive checks, but these are known
to be biased for predictive performance because this involves in-sample data (despite its name).
Posterior predictive checks are mainly used for assessing the in-sample fit of a generative model
to the data.

However, because we are providing full distributional predictions an argument
can be made that naturally, the model (when correctly specified) will at least have the 
ability to give an interval of the true, underlying relativity for a future
policyholder. While the model may still not assign a high mass to the true underlying relativity
in some cases (which, if we had the ideal dataset we could detect this) an argument could be made
that the true relativity was at least a possibility described by the model (say in a credible interval), something other traditional frequentist/machine learning methodologies do not have.

## Stan Code

Below is the exact same model I described above. In this case, I set $\tau$ and $\omega$ to 5 which are "weakly informative" priors. I don't really have strong apriori beliefs regarding the effects but 
I still want enough regularization to shrink effects to 0, if it is justifiable. 

For the variance parameter, I set $\alpha$ to 8. Again, this leads to a weakly informative prior, but in general I wanted to allow for the possibility of large conditional variances for particular rating combinations. Large losses likely have large dispersions, and some rating combinations may even have extreme dispersions depending on the observed data. 

```{stan output.var = "poisson_gamma_model"}
data {
  int<lower=1> Nobs;
  int<lower=1> Nvar;
  matrix[Nobs, Nvar] X;
  int<lower=0> claim_count[Nobs];
  vector<lower=0>[Nobs] exposure;
  vector<lower=0>[Nobs] severity;
}

parameters {
  vector[Nvar] beta_frequency;
  vector[Nvar] beta_severity;
  real<lower=0> dispersion_severity;
}

model {
  
  beta_frequency ~ normal(0, 5);
  beta_severity ~ normal(0, 5);
  dispersion_severity ~ cauchy(0, 8);
  
  claim_count ~ poisson_log(X * beta_frequency + log(exposure));
  
  for (i in 1:Nobs) {
    if (claim_count[i] != 0) {
      severity[i] ~ gamma(
      	claim_count[i] / dispersion_severity,
      	claim_count[i] / (exp(X[i] * beta_severity) * dispersion_severity)
      	);
    }
  }
  
}

```

Next, I preprocess the data in a suitable format for Stan. 

```{r Preprocessing}

data_with_severity <- data %>%
	mutate(
		severity = ifelse(claim_count != 0, claim_payments / claim_count, 0), 
		kilometres = as.factor(kilometres),
		zone = as.factor(zone),
		bonus = as.factor(bonus),
		make = as.factor(make),
		observed_pp = claim_payments / vehicle_exposure_years
	)

preprocessing <- recipe(claim_count ~ ., data = data_with_severity) %>%
	step_dummy(all_nominal())

prepped_recipe <- prep(preprocessing, training = data_with_severity)

X <- juice(prepped_recipe) %>%
	mutate(intercept = rep(1, nrow(.))) %>%
	relocate(intercept) %>%
	select(
		-vehicle_exposure_years,
		-claim_payments,
		-severity, 
		-claim_count, 
		-observed_pp
		) 

data_list <- list(
	Nobs = nrow(X),
	Nvar = ncol(X),
	X = as.matrix.data.frame(X),
	claim_count = data_with_severity$claim_count,
	exposure = data_with_severity$vehicle_exposure_years,
	severity = data_with_severity$severity
)

```

Now we sample from the posterior distribution as required:

```{r Sample from Posterior - Poisson/Gamma}

if (refit == TRUE) {
	fit_poisson_gamma <- sampling(
		poisson_gamma_model,
		data = data_list,
		chains = 6,
		iter = 6000,
		seed = 200350623,
		cores = 6,
		verbose = TRUE
		)
	
	saveRDS(fit_poisson_gamma, "rds/fittedPGmodel.rds")
} else {
	fit_poisson_gamma <- readRDS("rds/fittedPGmodel.rds")
}

```

This function extracts the required samples from the MCMC sampler.
```{r Helper Function For Extraction of Posterior Parameters}
#' Extract the required mean/dispersion parameters for both candidate frequency
#` and severity distributions.
#'
#' @param fitted_model A Stan fitted model (the result of rstan::sampling).
#`	This model MUST have named parameters beta_frequency, beta_severity, 
#`	and can optionally have parameters dispersion_frequency and 
#`  dispersion_severity.
#' @param exposures A vector of length N containing the exposures
#' @param X The design matrix of rating variables
#'
#' @return A list containing matrices/vectors of posterior samples of the 
#` required probability distributions.
#' @export
#'
#' @examples 
#` \dontrun{
#` extract_and_get_posterior_samples(
#` my_stan_fitted,
#` vehicle_exposure_years,
#` X_design
#` )
#`}
extract_and_get_posterior_samples <- function(fitted_model, exposures, X) {
	
	samples <- rstan::extract(fitted_model)
	exposure <- matrix(
		log(exposures),
		nrow = nrow(samples$beta_frequency),
		ncol = length(exposures),
		byrow = TRUE
		)
	
	if (is.null(samples$dispersion_frequency)) {
		dispersion_frequency_samples <- NULL
	} else {
		dispersion_frequency_samples <- samples$dispersion_frequency
	}
	
	if (is.null(samples$dispersion_severity)) {
		dispersion_severity_samples <- NULL
	} else {
		dispersion_severity_samples <- samples$dispersion_severity
	}
	
	list(
		frequency_samples = exp(
			samples$beta_frequency %*% t(as.matrix.data.frame(X)) + exposure
			),
		severity_samples = exp(
			samples$beta_severity %*% t(as.matrix.data.frame(X))
			),
		dispersion_frequency_samples = dispersion_frequency_samples,
		dispersion_severity_samples = dispersion_severity_samples,
		exposures = exposure
	)
	
}
```

We now extract the posterior samples.
```{r Extract Posterior Samples}
# Extract the samples for the Poisson-Gamma model.
pg_posterior_samples <- extract_and_get_posterior_samples(
	fitted_model = fit_poisson_gamma,
	exposures = data_with_severity$vehicle_exposure_years,
	X = X
)
```

## Posterior Predictive Checks

Does the underlying generative model fit the dataset well?

```{r Posterior Predictive Check function, echo = FALSE}
#' Generates posterior parameter samples for the weighted gamma.
#'
#' @param posterior_predictive_samples Matrix of posterior predictive samples.
#' @param y Vector of observations representing the actuals.
#' @param n Number of samples to plot. Must be less than the number of rows in
#'	posterior_predictive_samples.
#' @param variable Character string for graphing purposes.
#' @param limits Vector of upper and lower limits to plot (zooming in on the plot).
#'
#' @return None.
#' @export
#'
#' @examples
#' posterior_predictive_check(
#`	my_posterior_samples, n = 1000, variable = "claims"
#` )
posterior_pred_check <- function(posterior_predictive_samples, y, n, variable,
																 limits) {

	data_tibble <- as_tibble(posterior_predictive_samples[1:n, ]) %>%
		mutate(iter = 1:nrow(.)) %>%
		gather(key = "obs", value = "var", -iter) %>%
		select(-obs)
	
	ggplot(data_tibble, aes(x = var)) +
		geom_density(aes(group = as.factor(iter)), color = alpha("blue", 0.01)) +
		theme_bw() +
		scale_x_continuous(trans = "sqrt") +
		geom_density(
			data = tibble(actual = y),
			aes(x = actual), 
			size = 0.75, 
			alpha = 0.3
			) +
		coord_cartesian(xlim = limits, expand = FALSE) +
		labs(x = variable)

}
```

Note that these are misleading in the sense that they aren't actual assessments
of predictive performance, but of in-sample fit as discussed above. Still, if the 
model severely underfits the underlying data then the model will very likely not predict well. 

### Posterior Predictive Check - Frequency

First, I generate samples from the posterior predictive distribution as follows.
I then perform the posterior predictive check (i.e. compare the genrated data 
against the observed data).

```{r Generate Samples}

# Basically, flatten the matrix of frequencies to a vector, generate a vector 
# of length equal to this vector using each of these frequencies, then convert
# back
set.seed(200350623)
claim_count_posterior_pred <- rpois(
	n = length(pg_posterior_samples$frequency_samples),
	lambda = pg_posterior_samples$frequency_samples
	)

dim(claim_count_posterior_pred) <- dim(pg_posterior_samples$frequency_samples)
```

```{r Posterior Predictive Check - Frequency, fig.align="center", fig.height=6, fig.width=10}
posterior_pred_check(
	claim_count_posterior_pred,
	y = data$claim_count,
	n = 1000, 
	"claims (log2 scaled)",
	limits = c(0, 700)
	)
```

The blue curves represent 1000 simulated datasets generated from the posterior predictive.
The black curve is the actual, observed dataset (in this, claim counts). Ideally,
we want the black curve to fall **well** within the blue region as this implies
that the actual, observed dataset is a likely realization under our generative model.

The posterior predictive graphs are all zoomed in on the region where the density is not trivially 0 
for easier inspection.

Overall, we can see that the frequency model looks decent at most of the claim count
ranges (especially the lower claim counts), but the rating combinations
with larger claim counts seem to be more variable than what the model can explain.

That is, the distribution of claim counts exhibits possible overdispersion. Thus,
a possible improvement to the model might be to use the negative binomial distribution
instead, rather than the Poisson.

### Posterior Predictive Check - Severity

Similarily, we get posterior predictive draws of severities per each combination
of rating variables.

```{r}

#' Computes the posterior parameters for the weighted Gamma distribution.
#'
#' @param posterior_samples Posterior parameter samples from the Stan model.
#' @param posterior_predictive_claim_counts Posterior predictive claim counts.
#'  Must be a matrix.
#'
#' @return A list with posterior shape and rate parameters.
#' @export
#'
#' @examples sample_weighted_gamma(
#' posterior_samples, claim_counts_nb)
#'
#'
#'
sample_weighted_gamma <- function(
	posterior_samples,
	posterior_predictive_claim_counts) {
	
	dispersion_severity_posterior <- matrix(
		posterior_samples$dispersion_severity_samples,
		nrow = nrow(posterior_samples$frequency_samples),
		ncol = ncol(posterior_samples$frequency_samples)
		)
	
	shapes_posterior <- posterior_predictive_claim_counts / 
		dispersion_severity_posterior
	
	rates_posterior <- posterior_predictive_claim_counts /
		(posterior_samples$severity_samples * dispersion_severity_posterior)
	
	list(shapes = shapes_posterior, rates = rates_posterior)
	
	}

posterior_sev_samples_pg <- sample_weighted_gamma(
	pg_posterior_samples,
	claim_count_posterior_pred	
)

set.seed(200350623)
simulate_severity <- rgamma(
	n = length(pg_posterior_samples$severity_samples),
	shape = posterior_sev_samples_pg$shapes,
	rate = posterior_sev_samples_pg$rates
	)

dim(simulate_severity) <- dim(pg_posterior_samples$severity_samples)
```

```{r, fig.align="center", fig.height=6, fig.width=10}
posterior_pred_check(
	simulate_severity,
	y = data_with_severity$severity,
	n = 1000,
	variable = "severity (log2 scaled)",
	limits = c(0, 40000)
	)

```
</center>
As we can see in this case, the generative model looks off here. The model underfits the most important areas of largest density (the large spike) and fails to account for uncertainty at higher and lower levels 
of severity.

Unlike the frequency distribution, however, it is difficult to say if the gamma is simply inadequate here because this
distribution is dependent on how well the frequency distribution is defined as well. Fixing the issues with the frequency distribution could yield improvements here. In addition, simply choosing a more right skewed
distribution could be beneficial here.

Ultimately, however, the goal is the pure premium since our relativities are based
on these quantities. 

### Posterior Predictive Check - Pure Premium

```{r Posterior Predictive Check - PP}
simulate_pure_premium <- (
	claim_count_posterior_pred / exp(pg_posterior_samples$exposures)
	) * simulate_severity
```

```{r, echo = FALSE, fig.align="center", fig.height=6, fig.width=10}
posterior_pred_check(
	simulate_pure_premium,
	y = data_with_severity$observed_pp,
	n = 1000,
	variable = "pure premium (log2 scaled)",
	limits = c(0, 4000)
)

```

The "spikes" in the distribution are not as extreme as what is observed in the data. The black curve does not fall in the blue region in important areas of the distribution, namely the single peak. Thus, under the generative model the observed data is simply not probable in some regions of pure premium, which is an issue.

Otherwise, the generative model at least has the right shape, for the most part. 
## Model Revision - Switch Frequency to Negative Binomial

We noticed in the previous section that the Poisson distribution does not account for as much variance as we would like. Indeed, the variance of the Poisson distribution is equal to its mean. 

A negative binomial has variance greater than its mean and therefore could address issues
where uncertainty is understated for particular rating variable combinations with large claim counts.

The generative model is quite similar to the previous model, but we switch the frequency distribution:

$$X_{i} \big|N_{i}, \phi, \eta_{i} \sim Gamma\left(\frac{N_{i}}{\phi}, \frac{N_{i}}{\eta_{i} \times \phi}\right),$$

$$\eta_{i} \big| \beta, R_{i} = exp(R_{i} \times \beta),$$
$$N_{i} \big|\xi_{i}, \nu \sim \text{Negative Binomial}(\xi_{i}, \nu),$$
$$\xi_{i} \big|\psi, R_{i} = exp(R_{i} \times \psi + log(\text{exposure}_{i})),$$
$$\beta \sim Normal(0, 5),$$
$$\psi \sim Normal(0, 5),$$
$$\phi \sim \text{Half-Cauchy}(0, 8),$$
$$\nu \sim \text{Half-Cauchy}(0, 8).$$
Note that the Negative Binomial is using the mean, dispersion parameterization in Stan (i.e. NegativeBinomial2).

Compiling the model above in Stan:

```{stan output.var = "nb_gamma"}

data {
  int<lower=1> Nobs;
  int<lower=1> Nvar;
  matrix[Nobs, Nvar] X;
  int<lower=0> claim_count[Nobs];
  vector<lower=0>[Nobs] exposure;
  vector<lower=0>[Nobs] severity;
}

parameters {
  vector[Nvar] beta_frequency;
  vector[Nvar] beta_severity;
  real<lower=0> dispersion_severity;
  real<lower=0> dispersion_frequency;
}

model {
  
  beta_frequency ~ normal(0, 5);
  beta_severity ~ normal(0, 5);
  dispersion_severity ~ cauchy(0, 8);
  dispersion_frequency ~ cauchy(0, 8);
  
  claim_count ~ neg_binomial_2_log(
  	X * beta_frequency + log(exposure), dispersion_frequency
  );
  
  for (i in 1:Nobs) {
    if (claim_count[i] != 0) {
      severity[i] ~ gamma(
      	claim_count[i] / dispersion_severity,
      	claim_count[i] / (exp(X[i] * beta_severity) * dispersion_severity)
      	);
    }
  }
  
}
```

We again sample from the posterior distribution again:

Now we sample from the posterior distribution as required:

```{r Sample from Posterior - NB/Gamma}

if (refit == TRUE) {
	fit_nb_gamma <- sampling(
		nb_gamma,
		data = data_list,
		chains = 6,
		iter = 6000,
		seed = 200350623,
		cores = 6,
		verbose = TRUE
		)
	
	saveRDS(fit_nb_gamma, "rds/nb-weighted_gamma.rds")
} else {
	fit_nb_gamma <- readRDS("rds/nb-weighted_gamma.rds")
}

nb_posterior_samples <- extract_and_get_posterior_samples(
	fitted_model = fit_nb_gamma,
	exposures = data_with_severity$vehicle_exposure_years,
	X = X
)

```

## Posterior Predictive Check - Revision I