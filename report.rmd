---
title: "A Fully Bayesian Way of Estimating Insurance Relativities"
author: "Brayden Tang"
date: "27/11/2020"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(rstan)
library(tidyverse)
library(recipes)
library(DT)
library(kableExtra)
library(prettydoc)
```

## Some Context
I am not an actuarial student (anymore) but I recently completed a typical
pricing assignment as part of an evaluation for a job application.
I didn't end up applying, however I enjoyed having a (fake) familiar kind
of dataset that I used to work with all the time in my past life. I wanted to see
if I could get around some of the issues I had with these kinds of datasets
from three years ago.

The objective of actuarial relativity analysis is, at a high level, to most 
accurately predict pure premiums as closely as possible. The pure premium is 
the amount of money needed, on average, that is required to cover the cost of
claims only (so no profit allocations or expenses).

Naturally, pure premium is expressed simply as the rate at which a risk
makes any kind of claim multiplied by the average amount per each claim. Thus, pure premium
is defined as: $$ \text{frequency} \times \text{severity} = \frac{\text{total claim counts}}{\text{total risks = exposures}} \times \frac{\text{total claim amounts (losses)}}{\text{total claim counts}}  = \frac{\text{total claim amounts (losses)}}{\text{total risks}}.$$

We can model either the frequency and severity distributions separately, or we 
can model pure premium directly. The former approach is often preferred as 
it offers more flexibility and is at least logically, more robust. 

## Relativities

Relativities are simply the marginal impact a particular rating variable has on the
response. This has the exact same interpretation as regression coefficients do
in a multivariate linear model - holding all other variables constant, the regression
coefficient represents the change in some quantity (say, the log of severity or
the log of frequency) per some change in said rating variable. 

Relativities are typically calculated by calculating pure premiums across one
varying rating variable, where all of the other rating variables are held constant.
Then, we divide each pure premium per each level in the varied rating variable
by the base class premium to obtain relativities for this particular rating variable.

```{r relativities, echo = FALSE}

example_city_rels <- tibble(
	City = c("Small City", "Medium City", "Large City"),
	Relativity = c(0.95, 1, 1.50)
	) %>%
	kable() %>%
	kable_styling()

example_km_rels <- tibble(
	Kilometres = c("Less Than 10,000km", "10,000-50,000km", "50,001-100,000km", "More Than 100,000km"),
	Relativity = c(0.35, 0.76, 1, 1.378)
) %>%
	kable() %>%
	kable_styling()

knitr::kables(
	list(example_city_rels, example_km_rels),
	caption = "Figure 1: Example relativities with two rating variables") %>% 
	kable_styling() 

```
<br>
Note that the vehicle described by the rating variables with relativities 1 (medium city & 50,001-100,000km) is the base class (in a linear model, this is the class described by the intercept). Relative to the base class, other vehicles have higher or lower premiums. The base class premium is multiplied by the corresponding relativities
to derive the premiums for the other vehicle classes.

## The Dataset

I import the dataset below:

```{r Import the data}

data <- read_csv("data/pricingdat.csv") 

data %>% 
	datatable(
		rownames = FALSE,
		options = list(scrollY = 300, searching = FALSE)) %>%
	formatRound(columns = 6:7, digits = 0)

```
<br>
As is typical, one row represents one unique combination of rating variables, 
containing the total number of claims observed, amount of exposures (i.e. number of risks,
which is typically defined as the amount of policy earned over a period of one year) and
the total amount of claim payments. 

While this dataset seems straightforward, it is deceivingly difficult to work with when compared to 
other more standard datasets. This is primarily due to the aggregation which causes rows and
columns to be dependent. If we add more rating variables (which are columns) to the dataset, 
we naturally will get more rows since there will be more unique combinations of rating variables. 
In fact, the number of rows will grow exponentially (see the curse of dimensionality).

The consequences of this are dire. For one, model validation becomes impossible
just on this dataset alone. We can not naively split the aggregated dataset above because
each row is unique. Therefore, the test set will contain unique combinations of
rating variables that will be simply unobserved in the training set (and vice-versa).

Second, feature selection (which is emphasized a lot in pricing since variables
that have small or overfit effects directly impact the insured) and therefore, 
model selection, is impossible. We cannot simply drop columns without affecting the number of rows. Thus,
any sort of criteria based on the likelihood function (such as AIC/BIC/WAIC/LOO)
is not viable since the likelihood function naturally scales with the data. In addition,
since we cannot perform external validation we naturally cannot use wrapper based
methods (like forward selection) that rely on held out datasets.

Third, each row will naturally become more sparse as well, with some combinations of 
rating variables simply having zero claims or exposures. Naturally, this will
lead to ill-defined models - the traditional gamma won't work, for example, since
we will have exact zero losses. In the case of frequency distributions, there
may also be particular levels of a rating variable that simply have zero exposures.
This typically arises when a particular level that originally had some exposures many years ago decreases
over time to the point where it no longer has any exposures (suppose a vehicle color simply seizes to exist within a book of business). Normally, this level can simply be dropped, but
what happens when all of a sudden the exposure reappears in the future? Our data 
is a single realization of some reality, but there is of course **uncertainty**.
It could be by pure chance that we observed zero exposures.

Another solution to the above problem is to group the sparse level of the rating
variable with another to remove this sparsity. This seems harmless, but in reality
this corresponds to a deceivingly strong prior, namely that the
effects of each level on the response are the same. In math, this corresponds to setting $\beta_{1} - \beta_{2} \sim N(0, 2\epsilon)$ where $\epsilon$ is some number very close to 0, and $\beta_{1}, \beta_{2}$ are the two separate rating
variables being grouped. See [this video](https://www.youtube.com/watch?v=BKumW2RfSoQ&ab_channel=Stan) for more details. Regardless, grouping rating variables together is a very strong prior that
is implicit. Rather than doing this, we can use mixed effects/hierarchical models
instead to achieve pooling.

### Aside: The Ideal Dataset

The ideal dataset, at least hypothetically, would be a dataset where one row is 
equal to one policyholder. The total number of earned exposures for the policyholder
would be one column, and the total claim counts and total claim costs (for that
policyholder) would be additional columns (from which we could model). The rating
variables relevant for that policyholder would then be represented as additional
columns.

This would secure independence between the number of rating variables used
and the number of rows. To obtain relativities, all we have to do is then 
recreate the aggregated dataset (that is, one row is equal to one unique combination
of exposures) from which we can then make our predictions of frequency and severity
using our trained model (that was trained on the policyholder data). The model
predicts it's best guess of $E[Y | X = \text{rating variables}]$ learned over all
observed policyholders.

## Post-Adjustments

The model is not perfect and there is an acceptance that the models may be overfit
to the data, leading to relativites that are not reflective of the actual relativites.
This often leads to actuaries manually adjusting relativites to be more in line
with what their apriori beliefs are. For example, a vehicle with a larger engine would
be expected to be of higher risk than a vehicle with a smaller engine since presumably
the vehicles with larger engines are more expensive on average. Sometimes, the 
relativities produced do not reflect this and so an actuary may decide to "manually"
adjust the values to smooth said estimates. This is often done out of respect for the
customer who may not appreciate being charged more in premium when they perceive
themselves as lower risk. Depending on the actuary, they may just adjust relativities that are not monotonic. 

While this approach may sound sketchy, I believe there may actually be merit to it
in the sense that the relationship that is expected, or, a reasonable compromise to the prior belief
**is** actually in the data once we consider the amount of uncertainty that exists within the data. This is 
essentially what this manual adjustment is trying to account for, accepting that
the data has some degree of noise and may not be fully reliable. 

However, the adjustment is arbitrary if we do not actually know how much uncertainty we are 
dealing with. In the case of traditional pricing modeling, it is difficult to actually
quantify how much uncertainty we are dealing with because the frequency and severity
models are estimated completely independently of each other. The expectations produced
by each individual model are used to produce the pure premiums, which are then used to 
calculate relativities. 

However, it is clear that if we view frequency and severity as random variables
that they are not independent. If we have a zero frequency, we must have a zero severity (or equivalently,
if we have observed claims we must have non-zero severity, assuming that claims 
below a deductible are ignored). Estimating both models separately ignores this.

Viewing frequency and severity as random variables, and as a consequence, viewing pure premiums
and relativities as random variables, is where Bayesian modeling will allow us
to quantify the full amount of uncertainty in our data. This will allow actuaries to actually
choose reasonable values that are in line with the amount of uncertainty that they
have apriori and within the data. In addition, it allows actuaries to quantify 
their confidence in allowing for a non-monotonic relativity, giving them the ability
to defend any non-monotonic relativities.

## The Bayesian/Generative Model

I describe the fully generative model below:

Let $X$ be the severity random variable, $R_{i}$ be the rating variables,
and $N$ be the claim counts. Assume that exposures are a known constant. 

Then, for the ith unique combination of rating variables, $X_{i} > 0 \ \forall i$ let:

$$X_{i} \big|N_{i}, \phi, \beta \sim Gamma\left(\frac{N_{i}}{\phi}, \frac{N_{i}}{\eta_{i} \times \phi}\right),$$

$$\eta_{i} = exp(R_{i} \times \beta),$$
$$N_{i} \big|\lambda{i} \sim Poisson(\lambda_{i}),$$
$$\lambda_{i} = exp(R_{i} \times \psi + log(\text{exposure}_{i})),$$
$$\beta \sim Normal(0, \tau),$$
$$\psi \sim Normal(0, \omega),$$
$$\phi \sim \text{Half-Cauchy}(0, \alpha).$$
Note that $\alpha, \tau, \omega$ are constants that must be specified by the user.
In addition, note that $$E[X_{i} \big |N_{i}, \phi, \eta_{i}] = \frac{N_{i}}{\phi} \times \frac{\eta_{i} \times \phi}{N_{i}} = \eta_{i}$$ but $$Var[X_{i} \big |N_{i}, \phi, \eta_{i}] = \frac{N_{i}}{\phi} \times \frac{\eta_{i}^2 \times \phi^2}{N_{i}^2} = \frac{\eta_{i}^2 \times \phi}{N_{i}}.$$

That is, as the number of claims that makes up a severity calculation increases,
the lower the variance of the resulting severity distribution for the particular
combination of rating variables. Equivalently, the more claims for a particular 
combination of rating variables, the more influence that particular combination
of rating variables has on the overall model fit. This is pretty much the same
thing as using weights in a gamma generalized linear model, however, in this case
the weights are also random variables and everything is modeled simultaneously.

It follows that $$X_{i} \times \frac{N_{i}}{\text{number of risks for ith combination}} = P_{i},$$ where 
$P_{i}$ is the pure premium for the ith combination of rating variables, is also a random variable. Since
we will have joint posterior draws of the two random variables that the pure premium is a function of,
the posterior distribution of the pure premium (and relativities) is also known.

## Stan Code

Below is the exact same model I described above.

```{stan output.var = "poisson_gamma_model"}
data {
  int<lower=1> Nobs;
  int<lower=1> Nvar;
  matrix[Nobs, Nvar] X;
  int<lower=0> claim_count[Nobs];
  vector<lower=0>[Nobs] exposure;
  vector<lower=0>[Nobs] severity;
}

parameters {
  vector[Nvar] beta_frequency;
  vector[Nvar] beta_severity;
  real<lower=0> dispersion_severity;
}

transformed parameters {
  
}

model {
  
  beta_frequency ~ normal(0, 5);
  beta_severity ~ normal(0, 5);
  dispersion_severity ~ cauchy(0, 8);
  
  claim_count ~ poisson_log(X * beta_frequency + log(exposure));
  
  for (i in 1:Nobs) {
    if (claim_count[i] != 0) {
      severity[i] ~ gamma(
      	claim_count[i] / dispersion_severity,
      	claim_count[i] / (exp(X[i] * beta_severity) * dispersion_severity)
      	);
    }
  }
  
}

```



